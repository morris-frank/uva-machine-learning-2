\documentclass{article}
\usepackage{morris}
\usepackage{diagbox}
\usepackage{centernot}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{enumerate}

\newcommand{\indep}{\mathrel{\text{\scalebox{1.07}{\ensuremath{\perp\mkern-10mu\perp}}}}}
\newcommand{\nindep}{\centernot{\indep}}

\renewcommand{\thesection}{Problem \arabic{section}.}
\renewcommand{\thesubsubsection}{\alph{subsubsection})}

\DeclareMathOperator{\Bf}{B}

\def\usealphasub{0}
\renewcommand{\thesubsection}{
  \ifnum\usealphasub=1%
    \alph{subsection})
  \else
    \arabic{subsection}.
  \fi%
}

\newenvironment{alphasub}{%
  \def\usealphasub{1}
}{%
  \def\usealphasub{0}
}%


\title{Machine Learning 2 --- Homework 6}
\author{%
  Maurice Frank\\
  11650656\\
  \href{mailto:maurice.frank@posteo.de}{maurice.frank@posteo.de}
}

\begin{document}
\maketitle

\section{}
\begin{alphasub}
We have PDF \(p(\B{x}),\ \B{x}\∈\ℝ^d\) and the approximation of it \(q(\B{x})\).

\subsection{}
\begin{algorithm}
  \caption{Rejection Sampler}
  \begin{algorithmic}
    \Function{sample\_rejection}{$q,\ \tilde{p},\ c$}
      \State \(acc \gets False\)
      \While{\(acc == False\)}
        \State \(z_0 \gets \text{sample}(q(z))\)
        \State \(u \gets \text{rand}[0, c\·q(z_0)]\)
        \State \(acc \gets u \leq \tilde{p}(z)\)
      \EndWhile
      \State \Return \(z\)
    \EndFunction%
  \end{algorithmic}
\end{algorithm}

\subsection{}
Yes the generated samples are independent.
We sample independently from \(q(\B{z})\) which makes the accepted samples also independently.

\subsection{}
\begin{align*}
  w_n
  &= \÷{\nf{p(\B{z}_n)}{q(\B{z}_n)}}{\Σ_m \nf{p(\B{z}_m)}{q(\B{z}_m)}}
\end{align*}

\subsection{}
\begin{align*}
  \α(x_{t+1}, x_t)
  &= \min{\left(1, \÷{\tilde{p}(x_{t+1}) q(x_t|x_{t+1})}{\tilde{p}(x_t) q(x_{t+1}|x_t)}\right)}\\
  &= \min{\left(1, \÷{\tilde{p}(x_{t+1}) q(x_t|x_{t+1})}{\tilde{p}(x_t) q(x_{t+1})}\right)}\\
  &= \min{\left(1, \÷{\tilde{p}(x_{t+1}) q(x_{t+1}|x_t) q(x_t)}{\tilde{p}(x_t) {q(x_{t+1})}^2}\right)}\\
  &= \min{\left(1, \÷{\tilde{p}(x_{t+1}) q(x_t)}{\tilde{p}(x_t) q(x_{t+1})}\right)}\\
  &= \min{\left(1, \÷{p(x_{t+1}) q(x_t)}{p(x_t) q(x_{t+1})}\right)}\\
\end{align*}

\subsection{}
A proposed sample using the proposal distribution is chosing independent of the previous sample: \(q(x_{t+1}|x_t) = q(x_t)\).
Whether or not the proposal is added to the chain though, is dependent on the previous sample's value.
Thus two accepted successive samples in the chain are not independent of each other.

\subsection{}
When a proposal is rejected the independence sampler, samples the previous sample again:
\begin{align*}
  [x_1, x_1, x_3, x_4, x_4]
\end{align*}

\subsection{}
The rejection sampler compares the volumes between the proposal approximate distribution and the unnormalized true distribution.
With higher dimensionality the acceptance ratio will become exponentially small.
\B{TODO FINISH WRITING}

The importance sample
importance sampler: no

independence sampler: yes
\end{alphasub}

\section{}
We have \(x\sim \N(x|\μ,\τ^{-1})\), \(\μ\sim \N(\μ|\μ_0,s_0)\) and \(\τ\sim \text{Gamma}(\τ|a,b)\).

To gibbs sample from the posterior \(p(\μ,\τ|x)\) we iteratively sample from the conditionals \(p(\μ|\τ,x)\) and \(p(\τ|\μ,x)\).

\begin{align*}
  p(\μ|\τ,x)
  &= \N\left(\μ|\÷{\τ^{-1}}{s_0 + \τ^{-1}}\·\μ_0 + \÷{s_0}{s_0 + \τ^{-1}}\· x, \÷{1}{\÷{1}{s_0}+\÷{1}{\τ^{-1}}}\right)\\
  p(\τ|\μ,x)
  &= \text{Gamma}(\τ|a + \÷{1}{2}, b + \÷{1}{2}{(x - \μ)}{2})\\
\end{align*}
(using Bishop 2.141/142 and 2.150/151)

\section{}
\subsection{}
\begin{align*}
  p(\B{W},\B{Z},\B{\Θ},\B{\Φ}|\α,\β)
  &= p(\B{W}|\B{Z},\B{\Φ}) \· p(\B{Z}|\B{\Θ}) \· p(\B{\Φ}|\β) \· p(\B{\Θ}|\α)\\
  &= p(\B{W}|\B{\Φ}) \· p(\B{\Φ}|\β) \· p(\B{Z}|\B{\Θ}) \· p(\B{\Θ}|\α)\\
  p(\B{Z}|\B{\Θ}) \· p(\B{\Θ}|\α)
  &= \Π_d^D p(\B{z}_d|\B{\θ}_d) p(\B{\θ}_d|\α)\\
  &= \Π_d^D (\Π_n^{N_d} \Π_k^K p(z_{dn}=k|\θ_{dk})^{\δ(z_{dn}=k)}) p(\B{\θ}_d|\α)\\
  &= \Π_d^D (\Π_k^K \θ_{dk}^{A_{dk}}) \· (\÷{1}{\Bf(\α)} \Π_k^K \θ_{dk}^{\α-1})\\
  &= \Π_d^D \÷{1}{B(\α)} \Π_k^K \θ_{dk}^{A_{dk} + \α-1}\\
  p(\B{W}|\B{\Φ}) \· p(\B{\Φ}|\β)
  &= \Π_k^K (\Π_d^D \Π_n^{N_d} p(w_{dn}|z_{dn}, \φ_k)) \· p(\B{\φ}_k|\β)\\
  &= \Π_k^K (\Π_d^D \Π_n^{N_d} \Π_w^W p(w_{dn}=w|z_{dn}=k,\φ_{kw})^{\δ(w_{dn}=w)\δ(z_{dn}=k)}) \· \÷{1}{B(\β)} \Π_w^W \φ_{kw}^{\β-1}\\
  &= \Π_k^K \Π_w^W \φ_{kw}^{B_{kw}}, \φ_k \· \÷{1}{\Bf(\β)} \Π_w^W \φ_{kw}^{\β-1}\\
  &= \Π_k^K \÷{1}{\Bf(\β)} \Π_w^W \φ_{kw}^{\Bf_{kw} + \β-1}\\
  &\⇒\\
  p(\B{W},\B{Z},\B{\Θ},\B{\Φ}|\α,\β)
  &= \left(\Π_d^D \÷{1}{\Bf(\α)} \Π_k^K \θ_{dk}^{A_{dk} + \α-1}\right)
     \left(\Π_k^K \÷{1}{\Bf(\β)} \Π_w^W \φ_{kw}^{B_{kw} + \β-1}\right)\\
\end{align*}

\subsection{}
\begin{align*}
  \int p(\B{W},\B{Z},\B{\Θ},\B{\Φ}|\α,\β) d\B{\θ}_d
  &= \int \Π_d^D \÷{1}{\Bf(\α)} \Π_k^K \θ_{dk}^{A_{dk} + \α-1} d\B{\θ}_d\\
  &= \Π_d^D \÷{1}{\Bf(\α)} \int \Π_k^K \θ_{dk}^{A_{dk} + \α-1} d\B{\θ}_d\\
  &= \Π_d^D \÷{1}{\Bf(\α)} \int \÷{\Bf(\B{A}_{d}+\α)}{\Bf(\B{A}_{d}+\α)}\Π_k^K \θ_{dk}^{A_{dk} + \α-1} d\B{\θ}_d\\
  &= \Π_d^D \÷{\Bf(\B{A}_{d}+\α)}{\Bf(\α)} \int \Dir({\B{A}_{d} + \α}) d\B{\θ}_d\\
  &= \Π_d^D \÷{\Bf(\B{A}_{d}+\α)}{\Bf(\α)}\\
  \int p(\B{W},\B{Z},\B{\Θ},\B{\Φ}|\α,\β) d\B{\φ}_k
  &= \int \Π_k^K \÷{1}{\Bf(\β)} \Π_w^W \φ_{kw}^{B_{kw} + \β-1} d\B{\φ}_k\\
  &= \Π_k^K \÷{1}{\Bf(\β)} \int \Π_w^W \φ_{kw}^{B_{kw} + \β-1} d\B{\φ}_k\\
  &= \Π_k^K \÷{\Bf(\B{B}_k + \β)}{\Bf(\β)} \int \÷{1}{\Bf(\B{B}_k + \β)} \Π_w^W \φ_{kw}^{B_{kw} + \β-1} d\B{\φ}_k\\
  &= \Π_k^K \÷{\Bf(\B{B}_k + \β)}{\Bf(\β)} \int \Dir(\B{B}_k + \β) d\B{\φ}_k\\
  &= \Π_k^K \÷{\Bf(\B{B}_k + \β)}{\Bf(\β)}\\
\end{align*}

\subsection{}
\begin{align*}
  z_{di} &\sim p(z_{di} | z_{d\{N\setminus i\}}, \B{W}, \α, \β)\\
  p(\B{Z},\B{W},\α,\β)
  &= \left(\Π_d^D \÷{\Bf(\B{A}_{d}+\α)}{\Bf(\α)}\right)
     \left(\Π_k^K \÷{\Bf(\B{B}_k + \β)}{\Bf(\β)}\right)\\
\end{align*}

\begin{align*}
  p(z_{di} | z_{d\{N\setminus i\}}, \B{W}, \α, \β)
  &= \÷{p(\B{Z}_{d}, \B{W}_d, \α, \β)}{p(\B{Z}_{d\{N\setminus i\}}, \B{W}, \α, \β)}\\
  &= \÷{
      \÷{\Bf(\B{A}_{d}+\α)}{\Bf(\α)} (\Π_k^K \÷{\Bf(\B{B}_k + \β)}{\Bf(\β)})
    }{
      \÷{\Bf(\B{A}_{d,K\setminus i}+\α)}{\Bf(\α)} (\Π_k^{K} \÷{\Bf(\B{B}_{k,W\setminus i} + \β)}{\Bf(\β)})
    }\\
  &= \÷{
      \Bf(\B{A}_{d}+\α) (\Π_k^K \Bf(\B{B}_k + \β))
    }{
      \Bf(\B{A}_{d,K\setminus i}+\α) (\Π_k^{K} \Bf(\B{B}_{k,W\setminus i} + \β))
    }\\
\end{align*}

\section{}
\begin{alphasub}
We have \(p(\B{x}|\B{\μ}) = \Π_{i=1}^D \μ_i^{x_i} {(1-\μ_i)}^{1-x_i}\), \(\μ_i \∈ [0,1]\), \(x_i \∈ \{0,1\}\).

\subsection{}
\begin{align*}
  \E[x_i]
  &= \Σ_{x_i} x_i \· p(x_i|\μ_oi)\\
  &= \Σ_{x_i} x_i \· \μ_i^{x_i} {(1-\μ_i)}^{1-x_i}\\
  &= \μ_i\\
  \E[\B{x}]
  &= \B{\μ}
\end{align*}

\subsection{}
\begin{align*}
  \cov(\B{x})
  &= \text{diag}(\B{\μ}^T(1-\B{\μ}))
\end{align*}

\subsection{}
Now we got an mixture: \(p(\B{x}|\B{\μ},\B{\π}) = \Σ_{k=1}^K \π_k p(\B{x}|\B{\μ}_k)\).
And therefore now:
\begin{align*}
  \E[\B{x}]
  &= \Σ_{k=1}^K \π_k \B{\μ}_k
\end{align*}

\subsection{}
\begin{align*}
  \log p(\B{X}|\B{\μ},\B{\π})
  &= \log \Π_{n=1}^N p(\B{x}_n|\B{\μ},\B{\π})\\
  &= \log \Π_{n=1}^N \Σ_{k=1}^K \left( \π_k p(\B{x}_n|\B{\μ}_k) \right)\\
  &= \log \Π_{n=1}^N \Σ_{k=1}^K \left( \π_k \Π_{i=1}^D \μ_{ki}^{x_{ni}} {(1-\μ_{ki})}^{1-x_{ni}} \right)\\
  &= \Σ_{n=1}^N \log \left[\Σ_{k=1}^K \left( \π_k \Π_{i=1}^D \μ_{ki}^{x_{ni}} {(1-\μ_{ki})}^{1-x_{ni}} \right)\right]\\
\end{align*}

\subsection{}
A standard maximum likelihood approach would not work here as we have need to compute the logarithm of the sum.
We do not have a closed-form solution and the computation of this logarithm is difficult.

\subsection{}
For our variationl approach we have: \(p(\B{x}_n,\B{z}_n|\B{\μ},\B{\π}) = p(\B{z}_n|\B{\π})p(\B{x}_n|\B{z}_n,\B{\μ}) = \Π_{k=1}^K \π_k^{z_{nk}} p(\B{x}_n|\B{\μ}_k)^{z_{nk}}\).

The data log likelihood becomes:
\begin{align*}
  \log p(\B{X},\B{Z}|\B{\μ},\B{\μ})
  &= \log \Π_{n=1}^N p(\B{z}_n|\B{\π})p(\B{x}_n|\B{z}_n,\B{\μ})\\
  &= \Σ_{n=1}^N \log \Π_{k=1}^K \left( \π_k^{z_{nk}} p(\B{x}_n|\B{\μ}_k)^{z_{nk}} \right)\\
  &= \Σ_{n=1}^N \Σ_{k=1}^K \log \left( \π_k^{z_{nk}} p(\B{x}_n|\B{\μ}_k)^{z_{nk}} \right)\\
  &= \Σ_{n=1}^N \Σ_{k=1}^K \left( z_{nk}\· \log\π_k +  z_{nk}\· \log p(\B{x}_n|\B{\μ}_k) \right)\\
  &= \Σ_{n=1}^N \Σ_{k=1}^K \left[ z_{nk}\· \log\π_k +  z_{nk}\· \log \left( \Π_{i=1}^D \μ_{ki}^{x_{ni}} {(1-\μ_{ki})}^{1-x_{ni}} \right)\right]\\
  &= \Σ_{n=1}^N \Σ_{k=1}^K \left[ z_{nk}\· \log\π_k +  z_{nk}\· \Σ_{i=1}^D \left( x_{ni}\·\log\μ_{ki} + (1-x_{ni})\·\log (1-\μ_{ki}) \right)\right]\\
\end{align*}

\subsection{}
\begin{center}
  \begin{tikzpicture}
    \node[obs]                   (x_ni) {\(x_{ni}\)};
    \node[latent, above=of x_ni] (z_nk) {\(z_{n}\)};
    \node[const, right=of z_nk]  (pi)   {\(\B{\π}_k\)};
    \node[const, right=of x_ni]  (mu)   {\(\B{\μ}_k\)};

    \edge {z_nk} {x_ni};
    \edge {pi}   {z_nk};
    \edge {mu}   {x_ni};

    \plate {D}  {(x_ni)}    {\(i=1,\cdots, D\)};
    \plate {N}  {(D)(z_nk)} {\(n=1,\cdots, N\)};
    \plate {K}  {(pi)(mu)}  {\(k=1,\cdots, K\)};
  \end{tikzpicture}
\end{center}

\newcommand{\cB}{\ensuremath{\mathcal{B}}}
\subsection{}
\begin{align*}
  \cB(\{q_n(\B{z}_n)\},\B{\μ},\B{\π})
  &= \Σ_{n=1}^N H(q_n) + \Σ_{n=1}^N \E_{q_n}[\ln p(\B{x}_n, \B{z}_n|\B{\μ},\B{\π})]\\
  &= -\Σ_{n=1}^N \Σ_{\B{z}_k} q_n(\B{z}_n) \log q_n(\B{z}_n) + \Σ_{n=1}^N q_n(\B{z}_n) \ln p(\B{x}_n, \B{z}_n|\B{\μ},\B{\π})\\
  &= \Σ_{n=1}^N \Σ_{\B{z}_k} q_n(\B{z}_n) \left( \Σ_{k=1}^K \left[ z_{nk} \log\π_k +  z_{nk}\· \Σ_{i=1}^D \left( x_{ni}\log\μ_{ki} + (1-x_{ni})\log (1-\μ_{ki}) \right)\right] - \log q_n(\B{z}_n) \right)
\end{align*}

\subsection{}
\begin{align*}
  \tilde{\cB}(\{q_n(\B{z}_n)\}, \B{\μ}, \B{\π})
  &= \Σ_{n=1}^N \Σ_{\B{z}_k} q_n(\B{z}_n) \left( \Σ_{k=1}^K \left[ z_{nk} \log\π_k +  z_{nk}\· \Σ_{i=1}^D \left( x_{ni}\log\μ_{ki} + (1-x_{ni})\log (1-\μ_{ki}) \right)\right]\right.\\
  &\quad\quad \left. - \log q_n(\B{z}_n) \right) + \α\·\left(\Σ_{k=1}^K \π_k - 1\right) + \Σ_{n=1}^N \λ_n \· \left(\Σ_{\B{z}_N} q_n(\B{z}_n) - 1\right)\\
\end{align*}

\subsection{}
\begin{align*}
  \pf{}{q_n}\tilde{\cB}(\{q_n(\B{z}_n)\}, \B{\μ}, \B{\π})
  &=
\end{align*}

\subsection{}
\end{alphasub}

\end{document}
