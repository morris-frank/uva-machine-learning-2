{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Independent Component Analysis\n",
    "\n",
    "### Machine Learning 2 (2019)\n",
    "\n",
    "* The lab exercises can be done in groups of two people, or individually.\n",
    "* The deadline is Tuesday September 17th, 16:59.\n",
    "* Assignment should be submitted through Canvas! Make sure to include your and your teammates' names with the submission.\n",
    "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file should be \"studentid1\\_studentid2\\_lab#\", for example, the attached file should be \"12345\\_12346\\_lab1.ipynb\". Only use underscores (\"\\_\") to connect ids, otherwise the files cannot be parsed.\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please ask.\n",
    "* Use __one cell__ for code and markdown answers only!\n",
    "    * Put all code in the cell with the ```# YOUR CODE HERE``` comment and overwrite the ```raise NotImplementedError()``` line.\n",
    "    * For theoretical questions, put your solution using LaTeX style formatting in the YOUR ANSWER HERE cell.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* Large parts of you notebook will be graded automatically. Therefore it is important that your notebook can be run completely without errors and within a reasonable time limit. To test your notebook before submission, select Kernel -> Restart \\& Run All."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literature\n",
    "In this assignment, we will implement the Independent Component Analysis algorithm as described in chapter 34 of David MacKay's book \"Information Theory, Inference, and Learning Algorithms\", which is freely available here:\n",
    "http://www.inference.phy.cam.ac.uk/mackay/itila/book.html\n",
    "\n",
    "Read the ICA chapter carefuly before you continue!\n",
    "\n",
    "### Notation\n",
    "\n",
    "$\\mathbf{X}$ is the $M \\times T$ data matrix, containing $M$ measurements at $T$ time steps.\n",
    "\n",
    "$\\mathbf{S}$ is the $S \\times T$ source matrix, containing $S$ source signal values at $T$ time steps. We will assume $S = M$.\n",
    "\n",
    "$\\mathbf{A}$ is the mixing matrix. We have $\\mathbf{X} = \\mathbf{A S}$.\n",
    "\n",
    "$\\mathbf{W}$ is the matrix we aim to learn. It is the inverse of $\\mathbf{A}$, up to indeterminacies (scaling and permutation of sources).\n",
    "\n",
    "$\\phi$ is an elementwise non-linearity or activation function, typically applied to elements of $\\mathbf{W X}$.\n",
    "\n",
    "### Code\n",
    "In the following assignments, you can make use of the signal generators listed below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import sys\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed\"\n",
    "\n",
    "# Signal generators\n",
    "def sawtooth(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return (((x / period - phase - 0.5) % 1) - 0.5) * 2 * amp\n",
    "\n",
    "def sine_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return np.sin((x / period - phase) * 2 * np.pi) * amp\n",
    "\n",
    "def square_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return ((np.floor(2 * x / period - 2 * phase - 1) % 2 == 0).astype(float) - 0.5) * 2 * amp\n",
    "\n",
    "def triangle_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return (sawtooth(x, period, 1., phase) * square_wave(x, period, 1., phase) + 0.5) * 2 * amp\n",
    "\n",
    "def random_nonsingular_matrix(d=2):\n",
    "    \"\"\"\n",
    "    Generates a random nonsingular (invertible) matrix of shape d*d\n",
    "    \"\"\"\n",
    "    epsilon = 0.1\n",
    "    A = np.random.rand(d, d)\n",
    "    while abs(np.linalg.det(A)) < epsilon:\n",
    "        A = np.random.rand(d, d)\n",
    "    return A\n",
    "\n",
    "def plot_signals(X, title=\"Signals\"):\n",
    "    \"\"\"\n",
    "    Plot the signals contained in the rows of X.\n",
    "    \"\"\"\n",
    "    figure()\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        ax = plt.subplot(X.shape[0], 1, i + 1)\n",
    "        plot(X[i, :])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.suptitle(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates some toy data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "num_sources = 5\n",
    "signal_length = 500\n",
    "t = linspace(0, 1, signal_length)\n",
    "S = np.c_[sawtooth(t), sine_wave(t, 0.3), square_wave(t, 0.4), triangle_wave(t, 0.25), np.random.randn(t.size)].T\n",
    "plot_signals(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Make mixtures (5 points)\n",
    "Write a function `make_mixtures(S, A)' that takes a matrix of source signals $\\mathbf{S}$ and a mixing matrix $\\mathbf{A}$, and generates mixed signals $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.1 Make mixtures\n",
    "def make_mixtures(S, A):\n",
    "    return A @ S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test your function\n",
    "np.random.seed(42)\n",
    "A = random_nonsingular_matrix(d=S.shape[0])\n",
    "X = make_mixtures(S, A)\n",
    "plot_signals(X, \"Mixtures\")\n",
    "\n",
    "assert X.shape == (num_sources, signal_length), \"The shape of your mixed signals is incorrect\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Histogram (5 points)\n",
    "Write a function `plot_histograms(X)` that takes a data-matrix $\\mathbf{X}$ and plots one histogram for each signal (row) in $\\mathbf{X}$. You can use the `np.histogram()` (followed by `plot`) or `plt.hist()` function. \n",
    "\n",
    "Plot histograms of the sources and the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.2 Histogram\n",
    "def plot_histograms(X):\n",
    "    fig, axs = plt.subplots(1, X.shape[0], figsize=(16,3))\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.hist(X[i,:], fc=plt.cm.Set2(i), label=i)\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_histograms(S)\n",
    "plot_histograms(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of these distributions (sources or measurements) tend to look more like Gaussians? Can you think of an explanation for this phenomenon? Why is this important for ICA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 and 4 look most likely to a Gaussian, while 0 and 1 appear to be univariate the show a slight skew. 2 has a multivariate Histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Implicit priors (20 points)\n",
    "As explained in MacKay's book, an activation function $\\phi$ used in the ICA learning algorithm corresponds to a prior distribution over sources. Specifically, $\\phi(a) = \\frac{d}{da} \\ln p(a)$. For each of the following activation functions, *derive* the source distribution they correspond to.\n",
    "$$\\phi_0(a) = -\\tanh(a)$$\n",
    "$$\\phi_1(a) = -a + \\tanh(a)$$\n",
    "$$\\phi_2(a) = -a^3$$\n",
    "$$\\phi_3(a) = -\\frac{6a}{a^2 + 5}$$\n",
    "\n",
    "Give your answer without the normalizing constant, so an answer of the form $p(a) \\propto \\verb+[answer]+$ is ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "    p_0(a) &\\propto \\text{sech}(a) = \\frac{1}{\\cosh{(a)}}\\\\\n",
    "    p_1(a) &\\propto \\exp(-\\frac{1}{2} a^2 - \\ln{(\\frac{1}{\\cosh{a}})})\\\\\n",
    "    p_2(a) &\\propto \\exp(-\\frac{1}{4} a^4)\\\\\n",
    "    p_3(a) &\\propto (a^2 + 5)^{-3}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_0(a):\n",
    "    return -np.tanh(a)\n",
    "\n",
    "def p_0(a):\n",
    "    return 1 / np.cosh(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_1(a):\n",
    "    return -a + np.tanh(a)\n",
    "\n",
    "def p_1(a):\n",
    "    return np.exp(- 0.5 * a**2 - np.log(1 / np.cosh(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_2(a):\n",
    "    return -a**3\n",
    "\n",
    "def p_2(a):\n",
    "    return np.exp(-0.25 * a**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_3(a):\n",
    "    return -6*a / (a**2 + 5)\n",
    "\n",
    "def p_3(a):\n",
    "    return 1/(a**2 + 5)**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = [phi_0, phi_1, phi_2, phi_3]\n",
    "priors = [p_0, p_1, p_2, p_3]\n",
    "\n",
    "a = np.linspace(-5, 5, 1000)\n",
    "for prior in priors:\n",
    "    assert prior(a).shape == (1000, ), \"Wrong output shape\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the activation functions and the corresponding prior distributions, from $a = -5$ to $5$ (hint: use the lists defined in the cell above). Compare the shape of the priors to the histogram you plotted in the last question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.3 Implicit priors (continued)\n",
    "a = np.linspace(-5, 5, 1000)\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(16,8), sharex=True)\n",
    "axs = axs.flatten()\n",
    "for i, (ax1, ax2, act, pri) in enumerate(zip(axs[::2], axs[1::2], activation_functions, priors)):\n",
    "    for ax, fn, name in ((ax1, act, 'activation'), (ax2, pri, 'prior')):\n",
    "        ax.plot(a, fn(a), label=name, c=plt.cm.Set2(i))\n",
    "        ax.set_title(fn.__name__)\n",
    "plt.xlim(-3,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Whitening (15 points)\n",
    "Some ICA algorithms can only learn from whitened data. Write a method `whiten(X)` that takes a $M \\times T$ data matrix $\\mathbf{X}$ (where $M$ is the dimensionality and $T$ the number of examples) and returns a whitened matrix. If you forgot what whitening is or how to compute it, various good sources are available online, such as http://courses.media.mit.edu/2010fall/mas622j/whiten.pdf. Your function should also center the data before whitening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.4 Whitening\n",
    "def whiten(X):\n",
    "    # Zero-center the data\n",
    "    X -= X.mean(axis=1)[:, None]\n",
    "    \n",
    "    Σ = np.cov(X)\n",
    "    Λ, Φ = np.linalg.eig(Σ)\n",
    "    _Λ = np.diag(1/np.sqrt(Λ))\n",
    "    \n",
    "    return (Φ @ _Λ @ Φ.T) @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test your function\n",
    "Xw = whiten(X)\n",
    "assert Xw.shape == (num_sources, signal_length), \"The shape of your mixed signals is incorrect\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Interpret results of whitening (10 points)\n",
    "Make 3 figures, one for the sources, one for measurements and one for the whitened measurements. In each figure, make $5 \\times 5$ subplots with scatter plots for each pair of signals. Each axis represents a signal and each time-instance is plotted as a dot in this space. You can use the `plt.scatter()` function. Describe what you see.\n",
    "\n",
    "Now compute and visualize the covariance matrix of the sources, the measurements and the whitened measurements. You can visualize each covariance matrix using this code:\n",
    "```python\n",
    "# Dummy covariance matrix C;\n",
    "C = np.eye(5)  \n",
    "ax = imshow(C, cmap='gray', interpolation='nearest')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "for dat, name in ((S, 'Sources'), (X, 'Measurements'), (Xw, 'Whitened Measurements')):\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(12, 12))\n",
    "    fig.suptitle(name)\n",
    "    for i, j in product(range(5),range(5)):\n",
    "        axs[i,j].scatter(dat[i, :], dat[j, :], c=mpl.colors.rgb2hex(plt.cm.Set2(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the signals independent after whitening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exept one pair (1,4) they have lost their dependence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Covariance (5 points)\n",
    "Explain what a covariant algorithm is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Independent Component Analysis (25 points)\n",
    "Implement the covariant ICA algorithm as described in MacKay. Write a function `ICA(X, activation_function, learning_rate)`, that returns the demixing matrix $\\mathbf{W}$. The input `activation_function` should accept a function such as `lambda a: -tanh(a)`. Update the gradient in batch mode, averaging the gradients over the whole dataset for each update. Make it efficient, so use matrix operations instead of loops where possible (loops are slow in interpreted languages such as python and matlab, whereas matrix operations are internally computed using fast C code). Experiment with the learning rate and the initialization of $\\mathbf{W}$. Your algorithm should be able to converge (i.e. `np.linalg.norm(grad) < 1e-5`) in within 10000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.7 Independent Component Analysis\n",
    "def ICA(X, activation_function, learning_rate=0.5, print_int=100, max_int=10000):\n",
    "    assert callable(activation_function)\n",
    "    \n",
    "    W, N, i =  0.001 * random_nonsingular_matrix(X.shape[0]), X.shape[1], 0\n",
    "    learning_rate *= 0.3\n",
    "    while True:\n",
    "        a = W @ X\n",
    "        z = activation_function(a)\n",
    "        x_ = W.T @ a\n",
    "        ΔW = W + (z @ x_.T)/N\n",
    "        W += learning_rate * ΔW\n",
    "        \n",
    "        tol = np.linalg.norm(ΔW)\n",
    "        if i % print_int == 0:\n",
    "            print(f'\\r{activation_function.__name__}, Step {i:5d}: {tol:.2e} ', end='')\n",
    "            sys.stdout.flush()\n",
    "        if tol < 1e-5 or np.isnan(tol) or i >= max_int:\n",
    "            print()\n",
    "            break\n",
    "        i += 1\n",
    "    return W\n",
    "\n",
    "W_est = ICA(Xw, phi_3, learning_rate=1)  # Compare with ICA(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will test your function so make sure it runs with only X and phi as input, and returns only W\n",
    "# Also it should converge for all activation functions\n",
    "\n",
    "W_estimates = [ICA(Xw, activation_function=phi, learning_rate=1.0) for phi in activation_functions]\n",
    "assert all([W_est.shape == (num_sources, num_sources) for W_est in W_estimates])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Experiments  (5 points)\n",
    "Run ICA on the provided signals using each activation function $\\phi_0, \\ldots, \\phi_3$ (or reuse `W_estimates`). Use the found demixing matrix $\\mathbf{W}$ to reconstruct the signals and plot the retreived signals for each choice of activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8 Experiments\n",
    "for φ in activation_functions:\n",
    "    W_est = ICA(Xw, φ, 1.0)\n",
    "    _Xw = W_est @ Xw\n",
    "    \n",
    "    fig, axs = plt.subplots(1, X.shape[0], figsize=(16,2))\n",
    "    fig.suptitle(φ.__name__)\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.plot(_Xw[i, :], c=plt.cm.Set2(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_whitening_make_a_difference():\n",
    "    # Does it make a difference (in terms of speed of convergence) \n",
    "    # if you whiten your data before running ICA?\n",
    "    \n",
    "    # Return True or False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(does_whitening_make_a_difference()) == bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Audio demixing (10 points)\n",
    "The 'cocktail party effect' refers to the ability humans have to attend to one speaker in a noisy room. We will now use ICA to solve a similar but somewhat idealized version of this problem. The code below loads 5 sound files and plots them.\n",
    "\n",
    "Use a random non-singular mixing matrix to mix the 5 sound files. You can listen to the results in your browser using `play_signals`, or save them to disk if this does not work for you. Plot histograms of the mixed audio and use your ICA implementation to de-mix these and reproduce the original source signals. As in the previous exercise, try each of the activation functions.\n",
    "\n",
    "Keep in mind that this problem is easier than the real cocktail party problem, because in real life there are often more sources than measurements (we have only two ears!), and the number of sources is unknown and variable. Also, mixing is not instantaneous in real life, because the sound from one source arrives at each ear at a different point in time. If you have time left, you can think of ways to deal with these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio, display, Markdown\n",
    "\n",
    "# Save mixtures to disk, so you can listen to them in your audio player\n",
    "def save_wav(data, out_file, rate):\n",
    "    scaled = np.int16(data / np.max(np.abs(data)) * 32767)\n",
    "    scipy.io.wavfile.write(out_file, rate, scaled)\n",
    "\n",
    "# Or play them in your browser\n",
    "def play_signals(S, sample_rate, title=\"Signals\"):\n",
    "    display(Markdown(title))\n",
    "    for signal in S:\n",
    "        display(Audio(signal, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio sources\n",
    "source_files = ['beet.wav', 'beet9.wav', 'beet92.wav', 'mike.wav', 'street.wav']\n",
    "wav_data = []\n",
    "sample_rate = None\n",
    "for f in source_files:\n",
    "    sr, data = scipy.io.wavfile.read(f, mmap=False)\n",
    "    if sample_rate is None:\n",
    "        sample_rate = sr\n",
    "    else:\n",
    "        assert(sample_rate == sr)\n",
    "    wav_data.append(data[:190000])  # cut off the last part so that all signals have same length\n",
    "\n",
    "# Create source and measurement data\n",
    "S_audio = np.c_[wav_data]\n",
    "plot_signals(S_audio)\n",
    "play_signals(S_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.9 Audio demixing\n",
    "A = random_nonsingular_matrix(S_audio.shape[0])\n",
    "X = make_mixtures(S_audio, A)\n",
    "\n",
    "W_est = ICA(X, phi_3, learning_rate=0.05)\n",
    "de_X = W_est @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_signals(de_X, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report your results. Using which activation functions ICA recovers the sources?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original sound samples were recoverable using `phi_3` and a `η = 0.05`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Excess Kurtosis (15 points)\n",
    "The (excess) kurtosis is a measure of 'peakedness' of a distribution. It is defined as\n",
    "$$\n",
    "\\verb+Kurt+[X] = \\frac{\\mu_4}{\\sigma^4} - 3 = \\frac{\\operatorname{E}[(X-{\\mu})^4]}{(\\operatorname{E}[(X-{\\mu})^2])^2} - 3\n",
    "$$\n",
    "Here, $\\mu_4$ is known as the fourth moment about the mean, and $\\sigma$ is the standard deviation.\n",
    "The '-3' term is introduced so that a Gaussian random variable has 0 excess kurtosis.\n",
    "We will now try to understand the performance of the various activation functions by considering the kurtosis of the corresponding priors, and comparing those to the empirical kurtosis of our data.\n",
    "\n",
    "#### 1.10.1 (10 points)\n",
    "First, compute analytically the kurtosis of the four priors that you derived from the activation functions before. You may find it helpful to use an online service such as [Wolfram Alpha](https://www.wolframalpha.com/) or [Integral Calculator](https://www.integral-calculator.com/) to (help you) evaluate the required integrals. Give your answer as both an exact expression as well as a numerical approximation (for example $\\frac{\\pi}{2} \\approx 1.571$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Include your answer here (you can use math.gamma if needed)\n",
    "def get_kurtosis():\n",
    "    # Return a list with 4 numbers / expressions\n",
    "    # return [0, 0, 0, 0]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check\n",
    "kurtosis = get_kurtosis()\n",
    "print (kurtosis)\n",
    "assert len(kurtosis) == 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10.2 (5 points)\n",
    "Now use the `scipy.stats.kurtosis` function, with the `fisher` option set to `True`, to compute the empirical kurtosis of the dummy signals and the real audio signals.\n",
    "\n",
    "Can you use this data to explain the performance of the various activation functions on the synthetic and real data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.10.2 Excess Kurtosis\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
